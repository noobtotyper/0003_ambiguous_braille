{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a724c1",
   "metadata": {},
   "source": [
    "# Braille preliminaries\n",
    "It is written in 2 columns of 3 rows.\n",
    "\n",
    "The dots are named 1 2 3 top to bottom on the left, 4 5 6 top to bottom on the right.\n",
    "\n",
    "I name them so that 123456 corresponds to BRAILE.\n",
    "\n",
    "I do the follwing encodings:\n",
    "- EACH: Summing all of the same dots in a word. So \"hi\" which is 125 24 becomes (1,2,0,1,1,0)\n",
    "- BRA: a word becomes (l,r) where l is the sum of the left dots (123) and r is the sum of the right dots (456)\n",
    "- BAIE: a word becomes (c,e) where c is the sum of all dots in the corners (1346) and e is the sum of all dots in the inn (25)\n",
    "- BI: a word becomes (t,c,b) where t is top (14), c is center (25), b is bottom (36)\n",
    "\n",
    "Then the decoding is ambiguous, and that is most of the fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f383fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# Read info from initial format\n",
    "def get_char_dots():\n",
    "    char_dots={}\n",
    "    with open(\"braille.txt\", \"r\") as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            sp=line.split(\"\\t\")\n",
    "            char_dots[sp[0]]=sp[2].rstrip()\n",
    "    #print(char_dots)\n",
    "    return char_dots\n",
    "\n",
    "# Aggregates dict values and turns them into keys\n",
    "# So {\"a\":1,\"b\":2,\"c\":1} becomes {1:[\"a\",\"c\"],2:[\"b\"]}\n",
    "def aggregate_dictionary(di):\n",
    "    ag_di={}\n",
    "    for k,v in di.items():\n",
    "        a=ag_di.get(v,[])\n",
    "        a.append(k)\n",
    "        ag_di[v]=a\n",
    "    return ag_di\n",
    "\n",
    "char_dots=get_char_dots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb64237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 6)\n",
      "[(3, 2), (4, 4), (6, 4), (3, 2), (8, 6), (4, 4), (1, 1), (7, 2), (14, 8)]\n",
      "[\"'t\", 'is', 'me', 'he', 'if', 'at', 'an', 'hi', 'em', 'eh', \"'ii\", 'na', 'cia', 'oi', 'aii']\n",
      "['and', 'my', 'was', 'saw', 'idea', 'joe', 'yo', 'jim', 'due', 'doc', 'aye', 'tie', 'dan', 'dna', 'fed']\n",
      "['out', 'come', 'been', 'too', 'mean', 'mr.', 'name', 'son', 'boy', 'kids', 'pay', 'dear', 'fun', 'game', 'read']\n",
      "[\"'t\", 'is', 'me', 'he', 'if', 'at', 'an', 'hi', 'em', 'eh', \"'ii\", 'na', 'cia', 'oi', 'aii']\n",
      "['these', 'being', 'while', 'times', 'hands', 'chance', 'sense', 'jesus', 'song', 'state', 'choice', 'meant', 'swear', 'ring', 'quit']\n",
      "['and', 'my', 'was', 'saw', 'idea', 'joe', 'yo', 'jim', 'due', 'doc', 'aye', 'tie', 'dan', 'dna', 'fed']\n",
      "[':', '-', 'i', 'c', 'e']\n",
      "['like', 'calm', 'bank', 'bear', 'pack', 'bro', 'male', 'alan', 'lisa', 'meal', 'mail', 'rob', 'fail', 'sale', 'sara']\n",
      "['business', 'telling', 'shouldn', 'parents', 'speaking', 'forgive', 'quickly', 'helping', 'flowers', 'language', 'national', 'letters', 'memories', 'officers', 'location']\n"
     ]
    }
   ],
   "source": [
    "# Cell specific to BRA (or leri)\n",
    "\n",
    "# Helper function to separate left from right\n",
    "left=set([\"1\",\"2\",\"3\"])\n",
    "right=set([\"4\",\"5\",\"6\"])\n",
    "def get_leri_tup(st):\n",
    "    le=0\n",
    "    ri=0\n",
    "    for char in st:\n",
    "        if char in left:\n",
    "            le+=1\n",
    "        elif char in right:\n",
    "            ri+=1\n",
    "    return (le,ri)\n",
    "\n",
    "#In this dict, first element of value is left, second is right\n",
    "char_leri_tups={k:get_leri_tup(v) for k,v in char_dots.items()}\n",
    "\n",
    "# Converting text to ambiguous code\n",
    "def sum_leri(word):\n",
    "    le=0\n",
    "    ri=0\n",
    "    for ch in word:\n",
    "        tup=char_leri_tups.get(ch,(-1000,-1000))\n",
    "        le+=tup[0]\n",
    "        ri+=tup[1]\n",
    "    return (le,ri)\n",
    "\n",
    "def text_to_leri(text):\n",
    "    text=text.lower().replace(\".\",\" .\").replace(\",\",\" ,\").replace(\":\",\" :\").replace(\";\",\" ;\").replace(\"?\",\" ?\").replace(\"!\",\" !\").replace(\"'\",\" '\")\n",
    "    converted=[]\n",
    "    for w in text.split():\n",
    "        leri=sum_leri(w)\n",
    "        if leri[0]>=0:\n",
    "            converted.append(leri)\n",
    "    return converted\n",
    "\n",
    "\n",
    "# Now preparing for the reverse, so that all pairs map to some words\n",
    "def get_words_leri():\n",
    "    words_leri={}\n",
    "    with open(\"words_en.txt\",\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            w=line.split()[0]\n",
    "            su=sum_leri(w)\n",
    "            if su[0]>=0:\n",
    "                words_leri[w]=su\n",
    "\n",
    "    return words_leri\n",
    "\n",
    "\n",
    "def plot_leri(words_leri):\n",
    "    le=[d[0]+random.random()/5 for d in words_leri.values()]\n",
    "    ri=[d[1]+random.random()/5 for d in words_leri.values()]\n",
    "    plt.scatter(le,ri,s=1)\n",
    "    plt.show()\n",
    "\n",
    "words_leri=get_words_leri()\n",
    "leri_words=aggregate_dictionary(words_leri)\n",
    "\n",
    "# It works like so\n",
    "print(words_leri[\"irene\"])\n",
    "\n",
    "text=\"Hi my name is Irene and I like flowers\"\n",
    "leri_list=text_to_leri(text)\n",
    "print(leri_list)\n",
    "for leri in leri_list:\n",
    "    print(leri_words[leri][:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc256c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 6)\n",
      "[(2, 3), (7, 1), (8, 2), (3, 2), (8, 6), (6, 2), (1, 1), (6, 3), (13, 9)]\n",
      "['he', 'hi', 'eh', 'i.', 'b.', 'ji', 'e.', 'je', 'bj', 'jb', '.i', 'ej', 'ij', 'ih', 'hb']\n",
      "['my', 'man', 'make', 'came', 'kick', 'adam', 'map', 'lack', 'bum', 'sack', 'pam', 'yu', 'kay', 'cuba', 'nam']\n",
      "['come', 'mean', 'any', 'name', 'pay', 'fun', 'buy', 'black', 'pick', 'sun', 'club', 'mark', 'nick', 'alex', 'lock']\n",
      "[\"'t\", 'is', 'if', 'at', 'aah', 'de', \"'ii\", 'aw', 'ed', 'id', 'el', 'le', 'oi', 'li', 'bo']\n",
      "['they', 'after', 'hello', 'being', 'went', 'later', 'watch', 'word', 'hurt', 'sort', 'state', 'swear', 'ring', 'idiot', 'trip']\n",
      "['and', 'use', 'face', 'case', 'jack', 'sake', 'bus', 'yo', 'arm', 'due', 'doc', 'fake', \"'all\", 'aye', 'dan']\n",
      "[';', 'i', 'e', 'b']\n",
      "['but', 'like', 'one', 'take', 'said', 'off', 'old', 'isn', 'care', 'dead', 'end', 'deal', 'met', 'bye', 'safe']\n",
      "['telling', 'strong', 'wedding', 'strange', 'forgive', 'forever', 'finished', 'forward', 'holding', 'helping', 'witness', 'flowers', 'willing', 'hanging', 'towards']\n"
     ]
    }
   ],
   "source": [
    "# Cell specific to BAIE (or corins)\n",
    "\n",
    "# Helper function to separate centers from corners\n",
    "# Why this distinction? If you were poking a paper with a machine that makes circles,\n",
    "# you'd get quarter circles for the corners and semicircles for the ins.\n",
    "# After drilling a word you only know how many of each you have.\n",
    "corners=set([\"1\",\"3\",\"4\",\"6\"])\n",
    "ins=set([\"2\",\"5\"])\n",
    "def get_corins_tup(st):\n",
    "    cor=0\n",
    "    inn=0\n",
    "    for char in st:\n",
    "        if char in corners:\n",
    "            cor+=1\n",
    "        elif char in ins:\n",
    "            inn+=1\n",
    "    return (cor,inn)\n",
    "\n",
    "#In this dict, first element of value is corners, second is ins\n",
    "char_corins_tups={k:get_corins_tup(v) for k,v in char_dots.items()}\n",
    "\n",
    "# Converting text to ambiguous code\n",
    "def sum_corins(word):\n",
    "    cor=0\n",
    "    inn=0\n",
    "    for ch in word:\n",
    "        tup=char_corins_tups.get(ch,(-1000,-1000))\n",
    "        cor+=tup[0]\n",
    "        inn+=tup[1]\n",
    "    return (cor,inn)\n",
    "\n",
    "def text_to_corin(text):\n",
    "    text=text.lower().replace(\".\",\" .\").replace(\",\",\" ,\").replace(\":\",\" :\").replace(\";\",\" ;\").replace(\"?\",\" ?\").replace(\"!\",\" !\").replace(\"'\",\" '\")\n",
    "    converted=[]\n",
    "    for w in text.split():\n",
    "        corins=sum_corins(w)\n",
    "        if corins[0]>=0:\n",
    "            converted.append(corins)\n",
    "    return converted\n",
    "\n",
    "\n",
    "# Now preparing for the reverse, so that all pairs map to some words\n",
    "def get_words_corins():\n",
    "    words_corins={}\n",
    "    with open(\"words_en.txt\",\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            w=line.split()[0]\n",
    "            su=sum_corins(w)\n",
    "            if su[0]>=0:\n",
    "                words_corins[w]=su\n",
    "\n",
    "    return words_corins\n",
    "\n",
    "\n",
    "def plot_corins(corins):\n",
    "    cor=[d[0]+random.random()/5 for d in corins.values()]\n",
    "    edg=[d[1]+random.random()/5 for d in corins.values()]\n",
    "    plt.scatter(cor,edg,s=1)\n",
    "    plt.show()\n",
    "\n",
    "words_corins=get_words_corins()\n",
    "corin_words=aggregate_dictionary(words_corins)\n",
    "#plot_corins(words_corins)\n",
    "\n",
    "# It works like so\n",
    "print(words_corins[\"irene\"])\n",
    "\n",
    "text=\"Hi my name is Irene and I like flowers\"\n",
    "corins_list=text_to_corin(text)\n",
    "print(corins_list)\n",
    "for corin in corins_list:\n",
    "    print(corin_words[corin][:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41095c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, 2)\n",
      "[(2, 3, 0), (4, 1, 3), (6, 2, 2), (2, 2, 1), (6, 6, 2), (5, 2, 1), (1, 1, 0), (4, 3, 2), (8, 9, 5)]\n",
      "['he', 'hi', 'eh', 'ji', 'je', 'bj', 'jb', 'ej', 'ij', 'ih', 'hb', 'bh']\n",
      "['my', 'bum', 'kay', 'axl', 'luc', 'anu', 'una', 'yak', 'lax', 'csu', 'sax', 'apu', 'mui', 'usc', 'kms']\n",
      "['come', 'mean', 'name', 'black', 'pick', 'nick', 'neck', 'main', 'papa', 'anna', 'amen', 'clan', 'scan', 'backs', 'dock']\n",
      "['is', 'at', \"'ii\", 'aw', 'el', 'le', 'oi', 'li', 'bo', 'si', 'se', 'ta', 'wa', 'es', 'c.']\n",
      "['after', 'watch', 'ring', 'idiot', 'stage', 'twice', 'large', 'chris', 'easier', 'birds', 'chest', 'trade', 'theme', 'helen', 'bored']\n",
      "['and', 'case', 'jack', 'doc', 'fake', 'dan', 'fan', 'dna', 'beam', 'hack', 'pad', 'alec', 'ciao', 'med', 'beck']\n",
      "['i', 'e', 'b']\n",
      "['like', 'one', 'take', 'old', 'isn', 'met', 'bye', 'eye', 'wake', 'ran', 'ball', 'weak', 'kate', 'tim', 'lisa']\n",
      "['strong', 'forever', 'witness', 'flowers', 'towards', 'airport', 'warrant', 'robbery', 'spirits', 'liberty', 'mothers', 'battery', 'require', 'relative', 'rhythm']\n"
     ]
    }
   ],
   "source": [
    "# Cell specific to BI (or tomibo)\n",
    "\n",
    "# Helper function to separate top middle bottom\n",
    "top=set([\"1\",\"4\"])\n",
    "mid=set([\"2\",\"5\"])\n",
    "bot=set([\"3\",\"6\"])\n",
    "\n",
    "def get_tomibo_tup(st):\n",
    "    to=0\n",
    "    mi=0\n",
    "    bo=0\n",
    "    for char in st:\n",
    "        if char in top:\n",
    "            to+=1\n",
    "        elif char in mid:\n",
    "            mi+=1\n",
    "        elif char in bot:\n",
    "            bo+=1\n",
    "    return (to,mi,bo)\n",
    "\n",
    "#In this dict, first element of value is top, second is middle, last is right\n",
    "char_tomibo_tups={k:get_tomibo_tup(v) for k,v in char_dots.items()}\n",
    "\n",
    "# Converting text to ambiguous code\n",
    "def sum_tomibo(word):\n",
    "    to=0\n",
    "    mi=0\n",
    "    bo=0\n",
    "    for ch in word:\n",
    "        tup=char_tomibo_tups.get(ch,(-1000,-1000,-1000))\n",
    "        to+=tup[0]\n",
    "        mi+=tup[1]\n",
    "        bo+=tup[2]\n",
    "    return (to,mi,bo)\n",
    "\n",
    "def text_to_tomibo(text):\n",
    "    text=text.lower().replace(\".\",\" .\").replace(\",\",\" ,\").replace(\":\",\" :\").replace(\";\",\" ;\").replace(\"?\",\" ?\").replace(\"!\",\" !\").replace(\"'\",\" '\")\n",
    "    converted=[]\n",
    "    for w in text.split():\n",
    "        tomibo=sum_tomibo(w)\n",
    "        if tomibo[0]>=0:\n",
    "            converted.append(tomibo)\n",
    "    return converted\n",
    "\n",
    "\n",
    "# Now preparing for the reverse, so that all pairs map to some words\n",
    "def get_words_tomibo():\n",
    "    words_tomibo={}\n",
    "    with open(\"words_en.txt\",\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            w=line.split()[0]\n",
    "            su=sum_tomibo(w)\n",
    "            if su[0]>=0:\n",
    "                words_tomibo[w]=su\n",
    "\n",
    "    return words_tomibo\n",
    "\n",
    "\n",
    "words_tomibo=get_words_tomibo()\n",
    "tomibo_words=aggregate_dictionary(words_tomibo)\n",
    "\n",
    "# It works like so\n",
    "print(words_tomibo[\"irene\"])\n",
    "\n",
    "text=\"Hi my name is Irene and I like flowers\"\n",
    "tomibo_list=text_to_tomibo(text)\n",
    "print(tomibo_list)\n",
    "for tomibo in tomibo_list:\n",
    "    print(tomibo_words[tomibo][:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358562df",
   "metadata": {},
   "source": [
    "# Story\n",
    "So we have Leri, Corin and Tomibo. Each of them has a braille typewriter that is very special in the way it works.\n",
    "\n",
    "For Leri, each time a key is pressed, it counts separately the number of dots on the left and the number of dots on the right. Those two values accumulate by summing on each keypress, and when space is entered, the only thing that remains from a word is a tuple (le,ri) where le are the total dots on the left and ri are the total dots on the right.\n",
    "\n",
    "For Corin it is similar. But what is count separately is the number of dots on the corners or the inner ones.\n",
    "\n",
    "Similar for Tomibo, now top, middle and bottom dots are counted separately.\n",
    "\n",
    "They communicate only through their typewriters, but as the narrator I will indicate who is speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc73102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_leri(text):\n",
    "    return \"Leri    \"+str(text_to_leri(text))\n",
    "\n",
    "def encode_corin(text):\n",
    "    return \"Corin   \"+str(text_to_corin(text))\n",
    "\n",
    "def encode_tomibo(text):\n",
    "    return \"Tomibo  \"+str(text_to_tomibo(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54addf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_leri(leri_list,options=15):\n",
    "    for i,leri in enumerate(leri_list):\n",
    "        print(i,leri,leri_words[leri][:options])\n",
    "\n",
    "def decode_corin(corin_list,options=15):\n",
    "    for i,corin in enumerate(corin_list):\n",
    "        print(i,corin,corin_words[corin][:options])\n",
    "        \n",
    "def decode_tomibo(tomibo_list,options=15):\n",
    "    for i,tomibo in enumerate(tomibo_list):\n",
    "        print(i,tomibo,tomibo_words[tomibo][:options])\n",
    "\n",
    "def str_to_list(st):\n",
    "    return ast.literal_eval(st)\n",
    "    \n",
    "def decode(line,options=30):\n",
    "    content=str_to_list(line[8:])\n",
    "    match line[:8]:\n",
    "        case \"Leri    \":\n",
    "            decode_leri(content,options)\n",
    "        case \"Corin   \":\n",
    "            decode_corin(content,options)\n",
    "        case \"Tomibo  \":\n",
    "            decode_tomibo(content,options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f4952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      " Nice to meet you, I am a bit ambiguous.\n",
      "\n",
      "Encoding:\n",
      "Leri    [(5, 5), (4, 3), (6, 5), (6, 5), (1, 0), (1, 1), (3, 1), (1, 0), (5, 3), (16, 8), (1, 2)]\n",
      "\n",
      "Decoding\n",
      "0 (5, 5) ['don', 'get', 'how', 'who', 'yes', 'hey', 'any', 'god', 'nice', 'week', 'side', 'dr.', 'dog', 'ten', 'date', 'hide', 'gay', 'jane', 'code', 'feed', 'dies', 'dean', 'egg', 'st.', 'jean', 'a.m.', 'woo', 'no.', 'net', 'gig']\n",
      "1 (4, 3) ['to', 'on', 'no', 'can', 'go', 'see', 'had', 'by', 'eat', 'kid', 'ain', 'bed', 'mad', 'act', 'age', 'tea', 'sad', 'cat', 'dr', 'ate', 'sec', 'ian', 'st', 'jam', 'bid', 'fee', \"'t-\", 'un', 'chi', 'tae']\n",
      "2 (6, 5) ['you', 'not', 'yeah', 'got', 'time', 'does', 'fine', 'kind', 'seen', 'once', 'used', 'meet', 'hand', 'damn', 'easy', 'mine', 'whoa', 'gun', 'feet', 'ride', 'wear', 'deep', 'cry', 'cute', 'yep', 'hadn', 'size', 'ideas', 'jeff', 'wash']\n",
      "3 (6, 5) ['you', 'not', 'yeah', 'got', 'time', 'does', 'fine', 'kind', 'seen', 'once', 'used', 'meet', 'hand', 'damn', 'easy', 'mine', 'whoa', 'gun', 'feet', 'ride', 'wear', 'deep', 'cry', 'cute', 'yep', 'hadn', 'size', 'ideas', 'jeff', 'wash']\n",
      "4 (1, 0) [',', 'a']\n",
      "5 (1, 1) [':', '-', 'i', 'c', 'e']\n",
      "6 (3, 1) [\"'s\", \"'m\", 'be', 'as', 'am', 'ah', 'ma', 'ha', 'r', 'p', 'v', 'au', 'fa', 'a-a', 'sa', 'ki', 'bi', 'bc', 'ke', 'ck', \"'ai\", 'kc', 'cb', 'ao', 'af', 'ua', \"'o\", 'ik', 'ek', 'oa']\n",
      "7 (1, 0) [',', 'a']\n",
      "8 (5, 3) ['she', 'him', 'his', 'man', 'big', 'came', 'job', 'bit', 'use', 'ago', 'face', 'case', 'each', 'jack', 'bet', 'law', 'tv', 'ben', 'max', 'fat', 'gas', 'adam', 'beg', 'hat', 'jake', 'fan', 'san', 'nah', 'cos', 'led']\n",
      "9 (16, 8) ['laughter', 'upstairs', 'although', 'challenge', 'prepared', 'disappear', 'grateful', 'reports', 'shoulder', 'policeman', 'criminals', 'invisible', 'promises', 'satellite', 'superman', 'november', 'envelope', 'thrilled', 'constable', 'harrison', 'troubled', 'february', 'pervert', 'unlikely', 'cleveland', 'sexually', 'landlord', 'relevant', 'corrupt', 'napoleon']\n",
      "10 (1, 2) ['.', 'd', 'j']\n",
      "\n",
      " ['laughter', 'upstairs', 'although', 'challenge', 'prepared', 'disappear', 'grateful', 'reports', 'shoulder', 'policeman', 'criminals', 'invisible', 'promises', 'satellite', 'superman', 'november', 'envelope', 'thrilled', 'constable', 'harrison', 'troubled', 'february', 'pervert', 'unlikely', 'cleveland', 'sexually', 'landlord', 'relevant', 'corrupt', 'napoleon', 'jerusalem', 'popcorn', 'artificial', 'crackling', 'prophet', 'accomplice', 'casualties', 'restless', 'submarine', 'spaceship', 'mumbling', 'vegetable', 'sponsor', 'battalion', 'clapping', 'resolved', 'apparent', 'penelope', 'bluffing', 'smelling', 'cultures', 'metaphor', 'marriages', 'chauffeur', 'parachute', 'crippled', 'occasional', 'margarita', 'liability', 'calculated', 'gibberish', 'allegiance', 'despicable', 'montreal', 'stalling', 'afterlife', 'alzheimer', 'puppets', 'offshore', 'reserves', 'supplied', 'composer', 'platinum', 'rooftop', 'espresso', 'starship', 'lifelong', 'parasites', 'sorcerer', 'cover-up', 'medallion', 'blooming', 'obsessive', 'implants', 'sheppard', 'cromwell', 'blushing', 'municipal', 'trooper', 'proctor', 'hiroshima', 'bartlett', 'trolley', 'separates', 'lancaster', 'doubtful', 'simplest', 'shelters', 'reptiles', 'healthier']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "text=\"Nice to meet you, I am a bit ambiguous.\"\n",
    "print(\"Text\\n\",text)\n",
    "\n",
    "print(\"\\nEncoding:\")\n",
    "l1=encode_leri(text)\n",
    "print(l1)\n",
    "\n",
    "print(\"\\nDecoding\")\n",
    "decode(l1)\n",
    "\n",
    "print(\"\\n\",leri_words[(16,8)][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdda5a",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "From less to more ambiguous: Tomibo < Corin < Leri\n",
    "\n",
    "Some care was put so that it is possible to decipher, but as for the difficulty, it is what it is.\n",
    "\n",
    "# And the story goes something like this\n",
    "\n",
    "Leri, Corin and Tomibo are roommates, and they have a pet turtle. Well had, because it just died.\n",
    "\n",
    "They want to figure out what happened to the turtle, and you can figure out what happened by decoding what they say.\n",
    "\n",
    "Tips:\n",
    "- The format of each line is \"Name____ [list]\" so 8 characters followed by a list.\n",
    "- You can just pass the line to the decode() function, and it automatically selects the person and converts the list to words using the ast.literal_eval() method.\n",
    "- The first line is probably empty so it was printed nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755acfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "Tomibo  [(1, 1, 0), (3, 2, 1), (4, 4, 3), (7, 7, 3), (2, 3, 1), (3, 3, 2), (8, 6, 6), (0, 1, 0), (11, 9, 6), (3, 3, 2), (8, 4, 4), (6, 3, 0), (3, 3, 4), (3, 2, 1), (3, 4, 2), (10, 5, 7), (0, 2, 1)]\n",
    "Leri    [(6, 3), (9, 6), (2, 4), (7, 3), (7, 5), (4, 3), (7, 5), (9, 3), (1, 2)]\n",
    "Corin   [(1, 1), (10, 5), (17, 9), (1, 2)]\n",
    "Leri    [(4, 2), (6, 5), (11, 4), (3, 3), (1, 0), (6, 5), (5, 2), (1, 0), (7, 5), (4, 2), (7, 5), (1, 2)]\n",
    "Corin   [(5, 2), (0, 1), (1, 1), (5, 3), (7, 4), (1, 2)]\n",
    "Tomibo  [(3, 4, 1), (5, 3, 5), (4, 4, 2), (5, 3, 1), (12, 11, 5), (4, 4, 2), (0, 2, 1), (3, 4, 2), (1, 1, 2), (11, 7, 6), (0, 2, 1)]\n",
    "Leri    [(5, 5), (4, 4), (5, 4), (8, 3), (5, 4), (4, 3), (4, 3), (17, 9), (9, 3), (2, 1)]\n",
    "Tomibo  [(1, 1, 0), (3, 3, 2), (2, 3, 1), (12, 10, 7), (11, 8, 5), (0, 1, 0), (2, 2, 1), (11, 8, 1), (5, 4, 2), (0, 2, 1)]\n",
    "Leri    [(1, 1), (5, 4), (3, 3), (15, 14), (12, 10), (1, 0), (9, 5), (8, 5), (1, 2)]\n",
    "Corin   [(1, 1), (4, 2), (7, 4), (14, 8), (0, 1), (10, 3), (6, 5), (9, 3), (5, 3), (1, 2)]\n",
    "Tomibo  [(4, 6, 2), (8, 5, 6), (12, 7, 9), (0, 1, 0), (4, 6, 2), (4, 6, 2), (4, 2, 5), (8, 6, 2), (0, 1, 2)]\n",
    "Corin   [(8, 5), (14, 7), (5, 2), (0, 1), (1, 1), (5, 3), (7, 4), (4, 3), (17, 10), (13, 9), (1, 2)]\n",
    "Leri    [(4, 2), (1, 1), (4, 4), (5, 4), (8, 3), (5, 4), (4, 3), (4, 3), (17, 9), (9, 3), (1, 2)]\n",
    "Tomibo  [(5, 3, 0), (4, 2, 5), (8, 6, 3), (12, 10, 5), (6, 7, 2), (0, 1, 2)]\n",
    "Leri    [(15, 7), (5, 5), (1, 0), (3, 3), (4, 4), (13, 9), (7, 3), (5, 2), (4, 2), (1, 1), (10, 9), (5, 4), (8, 7), (1, 2)]\n",
    "Tomibo  [(2, 1, 2), (0, 1, 0), (3, 4, 2), (1, 1, 2), (3, 3, 1), (3, 5, 1), (5, 7, 3), (0, 2, 1)]\n",
    "Corin   [(3, 3), (7, 4), (7, 4), (10, 4), (6, 3), (17, 10), (1, 2)]\n",
    "Tomibo  [(1, 1, 0), (3, 2, 1), (4, 4, 3), (8, 6, 3), (5, 2, 3), (6, 4, 4), (6, 9, 2), (0, 1, 0), (5, 2, 1), (3, 5, 1), (5, 7, 3), (5, 4, 5), (6, 4, 1), (0, 2, 1)]\n",
    "Corin   [(6, 5), (0, 1), (5, 3), (9, 2), (14, 8), (4, 3), (8, 3), (7, 4), (4, 2), (4, 5), (8, 7), (2, 1)]\n",
    "Leri    [(6, 5), (1, 0), (1, 1), (7, 4), (9, 4), (4, 2), (8, 3), (1, 0), (5, 7), (2, 1)]\n",
    "Tomibo  [(5, 5, 4), (4, 6, 2), (0, 1, 0), (4, 6, 2), (2, 2, 1), (3, 4, 3), (6, 2, 3), (4, 4, 3), (0, 1, 0), (4, 2, 5), (7, 5, 3), (2, 3, 1), (0, 2, 1)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e9b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomibo  [(1, 1, 0), (3, 2, 1), (4, 4, 3), (7, 7, 3), (2, 3, 1), (3, 3, 2), (8, 6, 6), (0, 1, 0), (11, 9, 6), (3, 3, 2), (8, 4, 4), (6, 3, 0), (3, 3, 4), (3, 2, 1), (3, 4, 2), (10, 5, 7), (0, 2, 1)]\n",
      "0 (1, 1, 0) ['i', 'e', 'b']\n",
      "1 (3, 2, 1) ['of', 'in', 'do', 'sea', 'lab', 'hm', 'lf', 'ali', 'en', 'ni', 'ne', 'ike', 'bp', 'ale', 'ct', 'pi', 'ds', 'lea', 'bao', 'sai', 'kg', 'abs', 'ip', \"'if\", 'lai', 'od', 'fo', 'isa', 'pe', 'kei']\n",
      "2 (4, 4, 3) ['not', 'now', 'won', 'last', 'own', 'live', 'yet', 'lose', 'boss', 'top', 'less', 'sell', 'wall', 'evil', 'risk', 'nor', 'size', 'joy', 'tall', 'hook', 'woke', 'pot', 'wave', 'salt', 'shy', 'laws', 'ron', 'bush', 'lois', 'pro']\n",
      "3 (7, 7, 3) ['believe', 'secret', 'rings', 'itself', 'faster', 'release', 'repeat', 'grant', 'carter', 'freeze', 'shadow', 'wings', 'swing', 'realise', 'wasted', 'idiots', 'sophie', 'berlin', 'bishop', 'harold', 'gloria', 'repair', 'helmet', 'pirate', 'phrase', 'sting', 'tasted', 'listed', 'elders', 'parish']\n",
      "4 (2, 3, 1) ['it', 'we', 'oh', 'er', 'ho', 're', 'jo', 'et', 'ew', 'd.', 'te', 'sh', \"'he\", 'ri', 'f.', 'br', 'wi', 'ti', 'tb', 'rb', 'ir', 'oj', \"'hi\", 'bt', 'hs', 'lj', 'js', 'hl', 'wb', 'sj']\n",
      "5 (3, 3, 2) ['was', 'saw', 'law', 'oil', 'ill', 'sat', 'leo', 'eve', 'kit', 'boo', 'sis', 'les', 'seo', 'pr', 'me.', 'lil', 'ole', 'tao', 'hub', 'rao', 'sob', 'bev', 'hui', 'biz', 'ive', 'zeb', 'ell', 'vie', 'zee', \"'are\"]\n",
      "6 (8, 6, 6) ['wouldn', 'system', 'natural', 'lonely', 'russian', 'bye-bye', 'tunnel', 'wounds', 'supper', 'rounds', 'purple', 'sources', 'evelyn', 'blossom', 'vanity', 'morons', 'stinky', 'courses', 'squeals', 'closure', 'rumbles', 'sustain', 'donuts', 'resumes', 'removal', 'exhaust', 'slumber', 'joyful', 'touchy', 'stumble']\n",
      "7 (0, 1, 0) [',']\n",
      "8 (11, 9, 6) ['apologize', 'recently', 'ordinary', 'centuries', 'valentine', 'ancestors', 'possessed', 'proposed', 'brand-new', 'promoted', 'collector', 'sensation', 'programs', 'contents', 'portland', 'removing', 'motivated', 'initially', 'illegally', 'collateral', 'ferguson', 'prospect', 'vomiting', 'submitted', 'weaknesses', 'fractures', 'mouthing', 'wallpaper', 'donatello', 'impotent']\n",
      "9 (3, 3, 2) ['was', 'saw', 'law', 'oil', 'ill', 'sat', 'leo', 'eve', 'kit', 'boo', 'sis', 'les', 'seo', 'pr', 'me.', 'lil', 'ole', 'tao', 'hub', 'rao', 'sob', 'bev', 'hui', 'biz', 'ive', 'zeb', 'ell', 'vie', 'zee', \"'are\"]\n",
      "10 (8, 4, 4) ['found', 'buddy', 'public', 'enemy', 'jimmy', 'mercy', 'punch', 'launch', 'dylan', 'sample', 'circus', 'sandy', 'fucker', 'funds', 'bounce', 'smoked', 'calvin', 'spicy', 'dixon', 'nypd', 'yankee', 'buffy', 'bombay', 'malone', 'excess', 'vacant', 'tucked', 'marcos', 'lucien', 'mascot']\n",
      "11 (6, 3, 0) ['dead', 'cage', 'deaf', 'chad', 'dice', 'fade', 'iced', 'chic', 'dade', 'fifa', 'ddd', 'cede', 'edda', 'dabba', 'aicha', 'gaad', 'dadi', 'agda', 'bacha', 'gada', 'ecch', 'fabia', 'fadi', 'feda', 'fff', 'addi', 'babic', 'gica', 'dida', 'deci']\n",
      "12 (3, 3, 4) ['out', 'our', 'zoo', \"'but\", 'i-i-i', 'was-', 'stu', 'viv', 'zhu', 'u0', 'sur', 'ust', 'up.', 'zev', 'tou', 'zbz', 'vve', 'vos', 'tsu', \"'ry\", 'wou', 'rus', 'tuo', 'sru', 'lov', 'vez', 'saw-', 'tus', 'vol', 'v2']\n",
      "13 (3, 2, 1) ['of', 'in', 'do', 'sea', 'lab', 'hm', 'lf', 'ali', 'en', 'ni', 'ne', 'ike', 'bp', 'ale', 'ct', 'pi', 'ds', 'lea', 'bao', 'sai', 'kg', 'abs', 'ip', \"'if\", 'lai', 'od', 'fo', 'isa', 'pe', 'kei']\n",
      "14 (3, 4, 2) ['let', 'sir', 'huh', 'sit', 'its', 'set', 'war', 'ooh', 'art', 'owe', 'bro', 'rat', 'rob', 'bow', 'raw', 'uhh', 'lit', 'toe', 'aww', 'rio', 'wes', 'hoo', 'til', 'iot', 'joo', 'sew', 'tis', 'est', 'woe', 'hiv']\n",
      "15 (10, 5, 7) ['actually', 'quickly', 'monkeys', 'sundays', 'casualty', 'playful', 'aquarium', 'coupons', 'humanly', 'tuscany', 'plummer', 'dumpty', 'murugan', 'muldoon', 'plummet', 'optimum', 'sunblock', 'frame-up', 'humdrum', 'foucault', 'gypsum', 'unmoved', 'subhuman', 'unkempt', 'match-up', 'explain-', 'nylund', 'frumpy', 'crumbly', 'unbound']\n",
      "16 (0, 2, 1) ['.', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in text.splitlines()[1:2]:\n",
    "    print(line)\n",
    "    decode(line)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-methods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
